{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download train, valid, test data\n",
    "!gdown --id 17eLq5Ng5yfbX9tLq2tdiOdscO0ea8xK5\n",
    "!gdown --id 1Y-68UagW14hwJXNyaR2HnML5jEwvjQlj\n",
    "!gdown --id 1P_HLLvGq15gsgDl5P6QYrHkTlkcytMdr\n",
    "\n",
    "# Download pretrained embedding\n",
    "!gdown --id 1-6eFo8BJxG-_tH-ErDFCvkbk9cqfatzr\n",
    "!unzip *\\zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('/content/train.csv')\n",
    "df_test = pd.read_csv('/content/test.csv')\n",
    "df_valid = pd.read_csv('/content/valid.csv')\n",
    "columns=['label', 'id1', 'id2', 'sentence1', 'sentence2']\n",
    "\n",
    "df_train = df_train.drop(columns=['Unnamed: 0'])\n",
    "df_test = df_test.drop(columns=['Unnamed: 0'])\n",
    "df_valid = df_valid.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "df_train.columns=columns\n",
    "df_test.columns=columns\n",
    "df_valid.columns=columns\n",
    "\n",
    "df_train.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot number of labels\n",
    "from matplotlib import pyplot as plt\n",
    "one, zero = df_train['label'].value_counts()\n",
    "\n",
    "plt.bar(['1','0'], [one, zero], width=0.5)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# Define embedding arguments\n",
    "embedding_dim = 100\n",
    "max_length = 17\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "\n",
    "\n",
    "# Text cleaner function\n",
    "def clean(dataframe):\n",
    "  dataframe[['cleanedSent1', 'cleanedSent2']] = None\n",
    "  for i in range(len(dataframe)):\n",
    "      tokens1 = word_tokenize(dataframe['sentence1'].iloc[i])\n",
    "      tokens2 = word_tokenize(dataframe['sentence2'].iloc[i])\n",
    "      # remove all tokens that are not alphabetic\n",
    "      dataframe['cleanedSent1'].iloc[i] = ' '.join([word for word in tokens1 if word.isalpha()])\n",
    "      dataframe['cleanedSent2'].iloc[i] = ' '.join([word for word in tokens2 if word.isalpha()])\n",
    "  return dataframe\n",
    "\n",
    "df_train = clean(df_train)\n",
    "df_test = clean(df_test)\n",
    "df_valid = clean(df_valid)\n",
    "df_train.head(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Text tokenization and sequence padding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def preprocess(input_list):\n",
    "  tokenizer = Tokenizer()\n",
    "  tokenizer.fit_on_texts(input_list)\n",
    "  word_index = tokenizer.word_index\n",
    "  vocab_size = len(word_index)\n",
    "  print(vocab_size)\n",
    "  sequences = tokenizer.texts_to_sequences(input_list)\n",
    "  padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "  return padded, word_index\n",
    "\n",
    "\n",
    "sent1Padded = preprocess(input_list=df_train['cleanedSent1'])[0]\n",
    "sent2Padded = preprocess(input_list=df_train['cleanedSent2'])[0]\n",
    "xtrain = [sent1Padded] + [sent2Padded]\n",
    "ytrain = df_train['label'].values\n",
    "\n",
    "sent1Padded = preprocess(input_list=df_test['cleanedSent1'])[0]\n",
    "sent2Padded = preprocess(input_list=df_test['cleanedSent2'])[0]\n",
    "xtest = [sent1Padded] + [sent2Padded]\n",
    "ytest = df_test['label'].values\n",
    "\n",
    "sent1Padded = preprocess(input_list=df_valid['cleanedSent1'])[0]\n",
    "sent2Padded = preprocess(input_list=df_valid['cleanedSent2'])[0]\n",
    "xvalid = [sent1Padded] + [sent2Padded]\n",
    "yvalid = df_valid['label'].values\n",
    "\n",
    "print(len(xtrain), len(xtrain[0]))\n",
    "print('size of train:{}, test:{}, valid:{}'.format(xtrain[0].shape,\n",
    "                                                   xtest[0].shape,\n",
    "                                                   xvalid[0].shape))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The PlotLosses function, plots the validation and training loss function during the\n",
    "#  training to give an insight of training the model\n",
    "import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.accuracy = []\n",
    "        self.val_accuracy = []\n",
    "\n",
    "        self.fig = plt.figure()\n",
    "\n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.accuracy.append(logs.get('accuracy'))\n",
    "        self.val_accuracy.append(logs.get('val_accuracy'))\n",
    "        self.i += 1\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        plt.plot(self.x, self.accuracy, label=\"accuracy\")\n",
    "        plt.plot(self.x, self.val_accuracy, label=\"val_accuracy\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "        print('Validation metrics:')\n",
    "        print('val_losses: {}'.format(round(self.val_losses[-1], 3)),\n",
    "        'val_accuracy: {}'.format(round(self.val_accuracy[-1], 3)) )\n",
    "\n",
    "plot_losses = PlotLosses()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Pretrained embedding\n",
    "import numpy as np\n",
    "\n",
    "emb_path = r'/content/w2v_100d.txt'\n",
    "\n",
    "word_index = preprocess(input_list=df_train['cleanedSent1'])[1]\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(emb_path, encoding='utf-8') as f:\n",
    "    content = f.readlines()\n",
    "    for line in content[1:]:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[i] = embedding_vector\n",
    "\n",
    "print('number of unique words in pretrained emb:', len(embeddings_index))\n",
    "print('vocab size:', vocab_size)\n",
    "print('number of unique words in current emb(extracted from pretrained emb):', embeddings_matrix.shape)\n",
    "print('number of sentences in corpus:', len(df_train))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# keras imports\n",
    "from keras.layers import Dense, Input, LSTM, GRU, Dropout, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "\n",
    "\n",
    "# Define attenstion class\n",
    "class attention(Layer):\n",
    "  global w, b\n",
    "  def __init__(self,**kwargs):\n",
    "    super(attention,self).__init__(**kwargs)\n",
    "\n",
    "  def build(self,input_shape):\n",
    "    global w, b\n",
    "    self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "    self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")\n",
    "    super(attention, self).build(input_shape)\n",
    "    w = self.W\n",
    "    b = self.b\n",
    "    return self.W, self.b\n",
    "\n",
    "  def weightss(self): # تابع مربوط به دریافت وزن های لایه توجه\n",
    "    global w, b\n",
    "    return w, b\n",
    "\n",
    "  def call(self,x):\n",
    "    et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n",
    "    at=K.softmax(et)\n",
    "    at=K.expand_dims(at,axis=-1)\n",
    "    output=x*at\n",
    "    return K.sum(output,axis=1)\n",
    "\n",
    "  def compute_output_shape(self,input_shape):\n",
    "    return (input_shape[0],input_shape[-1])\n",
    "\n",
    "  def get_config(self):\n",
    "    return super(attention,self).get_config()\n",
    "\n",
    "\n",
    "\n",
    "# Define model\n",
    "def defined_model(RNN_layer):\n",
    "\n",
    "  # Creating word embedding layer\n",
    "  embedding_layer = Embedding(nb_words, embedding_dim, weights=[embeddings_matrix],\n",
    "                              input_length=max_length, trainable=False)\n",
    "\n",
    "  # Attention\n",
    "  att = attention()\n",
    "\n",
    "  # Creating LSTM Encoder\n",
    "  if RNN_layer == 'BiLSTM':\n",
    "    lstm_layer = Bidirectional(LSTM(128, return_sequences=True))#, dropout=0, recurrent_dropout=0))\n",
    "  elif RNN_layer == 'LSTM':\n",
    "    lstm_layer = LSTM(128, return_sequences=True)#, dropout=0, recurrent_dropout=0))\n",
    "  elif RNN_layer == 'GRU':\n",
    "    lstm_layer = GRU(128, return_sequences=True)#, dropout=0, recurrent_dropout=0))\n",
    "  else:\n",
    "    raise Exception(\"\"\"\n",
    "    RNN_layer argument is not correct.\n",
    "      choose one of the following layers:\n",
    "      1- BiLSTM\n",
    "      2- LSTM\n",
    "      3- GRU\n",
    "      \"\"\")\n",
    "\n",
    "  # Creating LSTM Encoder layer for First Sentence\n",
    "  sequence_1_input = Input(shape=(max_length,), dtype='int32')\n",
    "  embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "  att_in1 = lstm_layer(embedded_sequences_1)\n",
    "  att_out1 = att(att_in1)\n",
    "\n",
    "  # Creating LSTM Encoder layer for Second Sentence\n",
    "  sequence_2_input = Input(shape=(max_length,), dtype='int32')\n",
    "  embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "  att_in2 = lstm_layer(embedded_sequences_2)\n",
    "  att_out2 = att(att_in2)\n",
    "\n",
    "\n",
    "  # Merging two attention encodes vectors from sentences\n",
    "  merged = concatenate([att_out1, att_out2])\n",
    "  merged = Dense(256, activation='relu')(merged)\n",
    "  merged = Dense(128, activation='relu')(merged)\n",
    "  preds = Dense(1, activation='sigmoid')(merged)\n",
    "\n",
    "  model = Model(inputs=[sequence_1_input, sequence_2_input], outputs=preds)\n",
    "  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "# چاپ خلاصه مدل\n",
    "defined_model(RNN_layer='BiLSTM').summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model = defined_model(RNN_layer='BiLSTM')\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "# std imports\n",
    "import time\n",
    "# import gc\n",
    "import os\n",
    "\n",
    "# Model fitting\n",
    "def fitting(RNN_layer, epoch=10, batch_size=128):\n",
    "  model = defined_model(RNN_layer = RNN_layer)\n",
    "\n",
    "  checkpoint_dir = r'/content/checkpoints/'\n",
    "  checkpoint_name = str(int(time.time())) + 'model-{epoch:02d}-{val_accuracy:.3f}.hdf5'\n",
    "\n",
    "  if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "  checkpoint = ModelCheckpoint(checkpoint_dir + checkpoint_name,\n",
    "                              monitor='val_accuracy',\n",
    "                              save_weights_only=True,\n",
    "                              save_best_only=False,\n",
    "                              mode='auto',\n",
    "                              save_freq= 'epoch')\n",
    "\n",
    "  history = model.fit(xtrain,\n",
    "                      ytrain,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=epoch,\n",
    "                      validation_data=(xvalid, yvalid),\n",
    "                      verbose=1,\n",
    "                      callbacks=[plot_losses, checkpoint])\n",
    "\n",
    "  print(\"Training Complete\\n\")\n",
    "  print('==============================================================\\n')\n",
    "\n",
    "  return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# paraphrase identification using trained model which has 'BiLSTM' layer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Train with 'BiLSTM' layer\n",
    "model = fitting(RNN_layer ='BiLSTM', epoch=20, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Model evaluation\n",
    "loss, accu = model.evaluate(xtest, ytest, verbose=1)\n",
    "print(\"Untrained model, loss: {:.2f}% - accuracy: {:.2f}%\\n\\n\".format(loss, (100 * accu)))\n",
    "\n",
    "\n",
    "prediction = model.predict(xtest, verbose=1)\n",
    "prediction = np.where(prediction>0.5, 1, 0)\n",
    "\n",
    "# Calc Accuraccy, precision, recal, F1-measure (Classification report)\n",
    "matrix = classification_report(ytest,prediction,labels=[1,0])\n",
    "print('\\nClassification report : \\n',matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}